{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7c71ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/josefinemertens/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe14c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files\n",
    "df_wellbeing = pd.read_csv('Transcribed videos containing well-being - Arkusz1.csv')\n",
    "df_non_wellbeing = pd.read_csv('Videos not containing well-being - Arkusz1.csv')\n",
    "\n",
    "# Extract only the \"Transcribed text\" column\n",
    "df_wellbeing_text = df_wellbeing[['Transcribed text']]\n",
    "df_non_wellbeing_text = df_non_wellbeing[['Transcribed text']]\n",
    "\n",
    "df_wellbeing_text.to_csv('Wellbeing_text.csv', index=False)\n",
    "df_non_wellbeing_text.to_csv('NonWellbeing_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21dee196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file saved to: Combined_transcribed_text_with_labels.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_wellbeing_text['label'] = 'well-being'\n",
    "df_non_wellbeing_text['label'] = 'non-well-being'\n",
    "\n",
    "# Combine both dataframes\n",
    "df_combined = pd.concat([df_wellbeing_text, df_non_wellbeing_text], ignore_index=True)\n",
    "\n",
    "df_combined.to_csv('Combined_transcribed_text_with_labels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a94c1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwb = pd.read_csv('Wellbeing_text.csv')\n",
    "\n",
    "dfnwb = pd.read_csv('NonWellbeing_text.csv')\n",
    "\n",
    "df = pd.read_csv('Combined_transcribed_text_with_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b09ec477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0b04697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/josefinemertens/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/josefinemertens/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/josefinemertens/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define the text processing functions\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'#', '', text)  # Remove hashtags but keep the text\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove special characters and numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove additional white spaces\n",
    "    return text\n",
    "\n",
    "def case_normalization(text):\n",
    "    return text.lower()\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # Combine default English stopwords with custom stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stopwords = {\n",
    "        \"a\", \"about\", \"above\", \"actually\", \"after\", \"again\", \"against\", \"ah\", \"all\", \"alright\", \"also\", \"always\", \"am\", \n",
    "        \"an\", \"and\", \"any\", \"anything\", \"are\", \"as\", \"at\", \"back\", \"basically\", \"be\", \"because\", \"been\", \"before\", \n",
    "        \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"come\", \"could\", \"different\", \"do\", \"does\", \"doing\", \n",
    "        \"don\", \"down\", \"during\", \"each\", \"etc\", \"etcetera\", \"every\", \"everyone\", \"everything\", \"feel\", \"few\", \"for\", \n",
    "        \"from\", \"further\", \"get\", \"go\", \"going\", \"got\", \"had\", \"has\", \"have\", \"having\", \"he\", \"hello\", \"her\", \"here\", \n",
    "        \"hers\", \"herself\", \"hey\", \"hi\", \"him\", \"himself\", \"his\", \"hmm\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \n",
    "        \"its\", \"itself\", \"ive\", \"just\", \"kind\", \"know\", \"like\", \"literally\", \"look\", \"make\", \"many\", \"maybe\", \"me\", \n",
    "        \"might\", \"more\", \"most\", \"much\", \"my\", \"myself\", \"need\", \"never\", \"no\", \"nor\", \"not\", \"now\", \"of\", \"off\", \n",
    "        \"oh\", \"okay\", \"on\", \"once\", \"one\", \"only\", \"or\", \"other\", \"others\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \n",
    "        \"own\", \"probably\", \"really\", \"right\", \"same\", \"say\", \"see\", \"she\", \"should\", \"so\", \"some\", \"someone\", \n",
    "        \"something\", \"sort\", \"such\", \"take\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \n",
    "        \"there\", \"these\", \"they\", \"thing\", \"things\", \"think\", \"this\", \"those\", \"though\", \"through\", \"throughout\", \n",
    "        \"time\", \"to\", \"too\", \"totally\", \"two\", \"uh\", \"under\", \"until\", \"up\", \"us\", \"very\", \"want\", \"was\", \"way\", \"we\", \n",
    "        \"well\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"within\", \n",
    "        \"without\", \"would\", \"yeah\", \"yes\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"youre\"\n",
    "    }\n",
    "    stop_words.update(custom_stopwords)\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def process_text(text):\n",
    "    cleaned = clean_text(text)\n",
    "    normalized = case_normalization(cleaned)\n",
    "    tokens = tokenize(normalized)\n",
    "    no_stopwords = remove_stopwords(tokens)\n",
    "    lemmatized = lemmatize(no_stopwords)\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Apply the functions to each dataframe\n",
    "df['Transcribed text'] = df['Transcribed text'].fillna('').apply(process_text)\n",
    "dfwb['Transcribed text'] = dfwb['Transcribed text'].fillna('').apply(process_text)\n",
    "dfnwb['Transcribed text'] = dfnwb['Transcribed text'].fillna('').apply(process_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a144862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed combined dataframe with labels\n",
    "df_combined_processed = df[['Transcribed text', 'label']].copy()\n",
    "df_combined_processed['Transcribed text'] = df['Transcribed text'].fillna('').apply(process_text)\n",
    "df_combined_processed.to_csv('Processed_combined.csv', index=False)\n",
    "\n",
    "# Save processed well-being dataframe\n",
    "dfwb_processed = dfwb[['Transcribed text']].copy()\n",
    "dfwb_processed['Transcribed text'] = dfwb['Transcribed text'].fillna('').apply(process_text)\n",
    "dfwb_processed.to_csv('Processed_wellbeing_text.csv', index=False)\n",
    "\n",
    "# Save processed non-well-being dataframe\n",
    "dfnwb_processed = dfnwb[['Transcribed text']].copy()\n",
    "dfnwb_processed['Transcribed text'] = dfnwb['Transcribed text'].fillna('').apply(process_text)\n",
    "dfnwb_processed.to_csv('Processed_non_wellbeing_text.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
