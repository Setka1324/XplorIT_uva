{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f00d3-0f24-4b16-9e16-e3f1e78d68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas spacy nltk contractions\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30763fb5-2882-4bcb-8f01-f3a2482948e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.24.3\n",
      "Uninstalling numpy-1.24.3:\n",
      "  Would remove:\n",
      "    /Users/josefinemertens/anaconda3/lib/python3.11/site-packages/numpy-1.24.3.dist-info/*\n",
      "    /Users/josefinemertens/anaconda3/lib/python3.11/site-packages/numpy/distutils/site.cfg\n",
      "Proceed (Y/n)? "
     ]
    }
   ],
   "source": [
    "   !pip uninstall numpy\n",
    "   !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1043de-d581-408d-bcb7-77608f0485a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "  !pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ce41b-0508-4f78-9222-ae54e7f0d163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from contractions import fix\n",
    "\n",
    "# Load Spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Normalize and preprocess text.\"\"\"\n",
    "    # Expand contractions\n",
    "    text = fix(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove irrelevant tags or notes\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    # Remove excess whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    \"\"\"Tokenize and lemmatize text while retaining stopwords and punctuation.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_space])\n",
    "\n",
    "def preprocess_dataset(file_path, output_path):\n",
    "    \"\"\"Preprocess dataset and save cleaned data.\"\"\"\n",
    "    # Load data without pyarrow\n",
    "    data = pd.read_csv(file_path, engine='python')  # Ensure compatibility without pyarrow\n",
    "    \n",
    "    # Remove duplicates\n",
    "    data = data.drop_duplicates(subset=['Transcribed text'])\n",
    "    \n",
    "    # Drop rows with excessive missing or irrelevant data\n",
    "    data = data.dropna(subset=['Transcribed text'])\n",
    "    \n",
    "    # Clean, tokenize, and lemmatize text\n",
    "    data['Processed text'] = data['Transcribed text'].apply(clean_text).apply(tokenize_and_lemmatize)\n",
    "    \n",
    "    # Save the cleaned data to a new CSV\n",
    "    data[['Processed text']].to_csv(output_path, index=False)\n",
    "    print(f\"Cleaned data saved to {output_path}\")\n",
    "\n",
    "# File paths for datasets\n",
    "file_path_1 = \"YT_Containing_WB.csv\"  # Replace with actual path\n",
    "file_path_2 = \"SM_NOT_Containing_WB.csv\"  # Replace with actual path\n",
    "output_path_1 = \"Clean_YT_Containing_WB.csv\"\n",
    "output_path_2 = \"Clean_SM_NOT_Containing_WB.csv\"\n",
    "\n",
    "# Preprocess both datasets\n",
    "preprocess_dataset(file_path_1, output_path_1)\n",
    "preprocess_dataset(file_path_2, output_path_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ece84ca-4286-4784-9341-794c020aa6ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontractions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fix\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load Spacy model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from contractions import fix\n",
    "\n",
    "# Load Spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Normalize and preprocess text.\"\"\"\n",
    "    # Expand contractions\n",
    "    text = fix(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove irrelevant tags or notes\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    # Remove excess whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    \"\"\"Tokenize and lemmatize text while retaining stopwords and punctuation.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_space])\n",
    "\n",
    "def preprocess_dataset(file_path, output_path):\n",
    "    \"\"\"Preprocess dataset and save cleaned data.\"\"\"\n",
    "    try:\n",
    "        # Load data without pyarrow\n",
    "        data = pd.read_csv(file_path, engine='python')  # Ensures compatibility without pyarrow\n",
    "        \n",
    "        # Remove duplicates\n",
    "        data = data.drop_duplicates(subset=['Transcribed text'])\n",
    "        \n",
    "        # Drop rows with excessive missing or irrelevant data\n",
    "        data = data.dropna(subset=['Transcribed text'])\n",
    "        \n",
    "        # Clean, tokenize, and lemmatize text\n",
    "        data['Processed text'] = data['Transcribed text'].apply(clean_text).apply(tokenize_and_lemmatize)\n",
    "        \n",
    "        # Save the cleaned data to a new CSV\n",
    "        data[['Processed text']].to_csv(output_path, index=False)\n",
    "        print(f\"Cleaned data saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# File paths for datasets\n",
    "file_path_1 = \"YT_Containing_WB.csv\"  # Replace with actual path\n",
    "file_path_2 = \"SM_NOT_Containing_WB.csv\"  # Replace with actual path\n",
    "output_path_1 = \"Clean_YT_Containing_WB.csv\"\n",
    "output_path_2 = \"Clean_SM_NOT_Containing_WB.csv\"\n",
    "\n",
    "# Create and preprocess both datasets\n",
    "preprocess_dataset(file_path_1, output_path_1)\n",
    "preprocess_dataset(file_path_2, output_path_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00976b-2f6d-4f39-8501-6422273c615f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1d5faa-8caa-412c-afb0-c28250fdb9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
